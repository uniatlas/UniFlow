# 概念 #
## 迁移学习 ##
迁移学习利用预训练模型，可以让人们用少量数据集或者较少的计算力得到顶尖的结果，是一种非常强大的技术。预训练模型此前会在相似的、更大的数据集上进行训练。由于模型无需从零开始学习，它可以比那些用更少数据和计算时间的模型得到更精确的结果。

# GDAS #
论文提出了一些神经架构搜索的方法，论文链接：https://github.com/D-X-Y/GDAS/blob/master/data/GDAS.pdf
## GDAS优势 ##
- 不同于之前 NAS 的基于 RL 与 EA 的方法，RL 与 EA 的反馈（reward）需要经过长时间的训练轨迹后才能得到，而 GDAS 的反馈是基于梯度、是即时的，并且在每次迭代中给出。
- GDAS没有使用整个DAG，而是在一次训练迭代中采样一个子图。此外，GDAS中的采样是可学习的，有助于找到更好的结构单元（cell）。
- GDAS 能够使用较少的 GPU 实现更好的性能。在CIFAR-10上，GDAS可以在几个GPU小时内完成一个搜索过程，发现一个鲁棒的神经网络，测试误差为2.82%。
## 相关工作 ##
**- 更少的计算量**：
大多数NAS方法可以分为两种模式:宏观搜索和微观搜索。宏搜索算法的目标是直接发现整个神经网络。宏观搜寻需要巨大的计算量，在如此大的搜索空间中搜索网络是困难和无效的。因此宏观搜索一般使得网络深度限制在 12 层以内。因为层数不够，导致相对于较深的 CNN 模型，精度会不够。相比之下,GDAS 能够通过堆积数十个已发现的结构单元，使网络更深，实现更高的精度。微搜索算法的目标是发现结构单元 cell，并通过将发现的神经细胞的多个副本堆叠起来，设计一个神经网络结构.一个典型的微型搜索方法是 NASNet，该方法能够在他们提出的 “NASNet 搜索空间”搜索结构单元.这些微型搜索算法通常需要 100 多天的 GPU 时间。GDAS也是一个微型搜索算法，专注于降低搜索成本。在实验中，我们可以在更短的GPU时间内找到一个健壮的网络，比标准NAS方法少1000倍。

**- 更加高效**：
因为NAS算法通常需要昂贵的计算资源，越来越多的研究人员关注于提高架构搜索速度。人们提出了多种技术，如渐进多路搜索阶段、精度预测、HyperNet、Net2Net transformation，参数共享等。GDAS以一种可区分的方式对各个体系结构进行采样，以有效地发现体系结构。因此，在CIFAR-10上，GDAS可以在几个GPU小时内完成搜索过程，这比这些高效的方法快得多。GDAS 重点是使采样过程可微和加快搜索过程。
## 原理 ##
**- 将搜索空间作为DAG：**
我们在搜索空间中搜索神经细胞，并将这些神经细胞串联起来构成整个神经网络。对于 CNN 来说，一个单元是一个完全卷积的网络，它以之前单元的输出张量为输入，生成另一个特征张量。对于递归神经网络(RNN)，单元格将当前步骤的特征向量和前一步的隐藏状态作为输入，生成当前隐藏状态。下面的样例就是 CNN 的例子。
![](https://i.imgur.com/lLKRazH.png)

**- 从 cell 到整个网络：**
从上图可以看出 DAG 主要搜索了 normal cell 和 reduction cell。对于 normal cell，F中的每个函数的步长为1。对于 reduction cell，F中的每个函数的步长为2.如上图所示对于 CIFAR 结构来说将 N 个 cell 组成了一个 block.如上图所示，给定一个图像，它首先通过网络头部分转发，即，一个3×3的卷积层。然后它向前通过三个块，中间有两个还原细胞

**- 利用可微模型抽样进行搜索：**
NAS 的目标就是找到一个最优的网络结构，

**- 关于 Reduction Cell 的讨论：**
大多数人工设计和自动发现的还原单元都很简单，可以达到很高的精度。此外，与搜索一个 normal cell 相比，联合搜索一个 normal cell 和一个 reduction cell 将大大增加搜索空间，使优化变得困难。下图展示了一个自己设计的 reduction cell,GDAS 搜索到了一个更好的结构并且具有更高的精度。
![](https://i.imgur.com/oOT63EJ.png)

## 讨论 ##
目前 NAS 都是现在小数据集上进行结构搜索然后再在大数据集上进行调整。如果直接在 ImageNet 上搜索的障碍是计算量大。GDAS 是一种高效的 NAS 算法，为我们提供了在 ImageNet 上进行搜索的机会。我们将在今后的工作中探索这一研究方向。
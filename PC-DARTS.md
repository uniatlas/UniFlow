# PC-DARTS 介绍 #
DARTS 在网络体系结构中仍然存在较大的冗余空间，存在较大的内存和计算开销。因为这限制了 DARTS 不能使用太大的 batch size,因此损失了搜索的速度或者高稳定性。有些论文提出减少搜索空间，这导致了一个近似，可能牺牲了发现的架构的最优性。PC-DARTS（Partially-Connected DARTS） 能够有效的减少显存，因为能够用更大的 batch 进行搜索，并且所花费的时间更少而且拥有更高的精度。PC-DARTS 是 DARTS 的衍生版。其核心的思想是：没有将所有通道都发送到操作选择块中，而是随机抽取其中的一个子集进行操作选择，同时直接绕过其余的通道。假设这个子集上的计算是对所有通道上的计算的近似。除了极大地降低了内存和计算成本外，抽样还带来了另一个好处，即操作搜索是规范化的，不太可能陷入局部最优。然而，PC-DARTS也带来了一个副作用，当不同的通道子集在迭代中采样时，网络连接的选择将变得不稳定。因此，我们引入边缘标准化，通过显式学习一组额外的边缘选择超参数来稳定网络连接搜索。通过在整个训练过程中共享这些超参数，所学习的网络体系结构更不容易跨迭代采样通道，从而更稳定。
得益于部分连接策略，我们能够相应地增加批大小。在实际操作中，我们对每个操作选择随机抽取1/K个通道作为样本，这几乎减少了K倍的内存开销。这使得我们可以在搜索过程中使用K×批大小，这不仅加快了搜索速度K倍，而且稳定了搜索，特别是对于大型数据集。
# 相关工作 #
现有NAS方法大致可以分为三类，即基于进化的方法、基于强化学习的方法和 one-shot 方法。
为了在较短的时间内完成架构搜索，研究人员考虑降低评估每个搜索候选对象的成本。早期的工作包括在搜索和新生成的网络之间共享权重，后来这些方法被概括为一个更优雅的框架，称为 one-shot 架构搜索。在这些方法中，只训练一次覆盖所有候选操作的超参数化网络或超网络，并从该超网络的采样中得到最终的体系结构。

在此图中解释了信息如何传输到 node3,在模型搜索的过程中主要有2组超参数：

# 方法论 #
- **DARTS：**
论文首先回顾了 DARTS：DARTS 将搜索到的网络分解为若干(L)个单元格。每个单元被组织成一个有 N 个节点的有向无环图(DAG)，其中每个节点定义一个网络层。有一个预定义的操作空间，用O表示，其中每个元素 O(·)是在网络层上执行的固定操作(如恒等连接、3×3卷积)。在单元格中，目标是从O中选择一个操作来连接每对节点。设一对节点为(i,j)，其中 DARTS 的核心思想是将从  i传播到 j 的信息表示为 |O| 操作的加权和。这种设计使得整个框架可以区分层权值和超参数 αo i, j,这样就可以以一个端到端的方式执行架构搜索。搜索过程结束后会保留相应的参数。
- **PC-DARTS：**
DARTS 的一个缺点是它的内存不足。为了适应|O|操作，在搜索架构的主要部分中，需要在每个节点(即，每个网络层)，导致|O|×内存使用。为了适应GPU，必须在搜索过程中减少批量大小，这不可避免地会降低搜索速度，而且更小的批量大小可能会降低搜索的稳定性和准确性。
以图一上看，以xi到xj的连接为例。这包括定义一个通道采样掩码si,j，它将1分配给选定的通道，0分配给掩码通道。所选信道被送入|O|运算的混合计算中，而掩蔽信道则绕过这些运算，即，它们被直接复制到输出，

- 
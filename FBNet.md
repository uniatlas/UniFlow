# FBNet 的特点 #
## 搜索空间 ##
本文建立一个层次级别的搜索空间，其宏观结构固定，每层可以选择不同的block.网络整体结构确定了网络层数和每层的输入输出的维数。网络的前三层和最后三层有着固定的操作。对于剩下的层，他们的 Block 需要被寻找。每层的卷积核个数是人为预先确定好的，对于浅层，使用相对较小的通道数，因为浅层的特征图分辨率较大，FLOP 与输入尺寸的二次幂成正比.
![](https://i.imgur.com/6eY5dM9.png)

网络中的每个可搜索层可以从层次搜索空间中选择不同的块，Block 的结构来源于 MobileNetv2 和 ShiftNet，如下图：包含 1X1 的 point-wise 卷积，KXK 的 depthwise 卷积，以及一个 1X1 的 point-wise 卷积，ReLU 函数跟在第一个 1X1 卷积和第二个 KXK 卷积之后，最后一个 1X1 卷积之后不接激活函数，如果输入分维度与输出分维度相同，则使用短路连接将输入与输出相加。同样仿照 MobileNet-v2 和 ShiftNet 以一个扩展比例 e 来控制 Block。它决定了第一个 1X1 卷积将输入特征图的通道扩大多少，对于 Depth-wise 卷积，卷积核的大小可以是 3X3 和 5X5，对于第一个和最后一个 1X1 的卷积，使用 group 卷积（point-wise）来降低计算量。当使用 group 卷积时，仿照 ShuffleNet 来进行通道信息融合。
![](https://i.imgur.com/XB3gPLS.png)
## 搜索算法 ##
本文首先将搜索空间表示为一个随机的超级网络。这个超级网络有着上表所示的整体结构，每层包含 9 个上表所示并行的 Blocks，在超级网络的前向过程中，只有一个候选的 Block 被采样进来.
在整个搜索过程中有2个参数需要关注一个是 w 即普通的网络权重，一个是关于随机采样概率的参数 theta
搜索过程等价于去训练随机super net。训练过程中，计算：损失对于权重的偏导数，去训练super net中每个操作 f 的权重，这部分与传统的卷积神经网络没有分别，操作搜索训练完城后，不同的操作使得整体网络的精度和速度不同，因此计算损失对于网络结构分布的偏导数，去更新每个操作的抽样概率 theta。这一步会使得网络整体的精度和速度更高，因为抑制了低精度和低速度的选择，super net训练完成之后，可以从结构分布 theta 中抽样获得最优的结构。
 ## 特点 ##
新方法DNAS：

- DNAS 使用基于梯度的方法，不同于之前 NAS 流派的基于 RL 的方法。
- DNAS 优化的是结构分布，不同于之前是直接寻找的结构。
- DNAS 的 loss 由交叉熵和延时两部分组成。
- 使用速查表去计算延时,延时是硬件相关的,之前用 FLOPs 度量速度。
延时关于层结构是可微的。

# 几点确认的点 #

- 网络是如何定义的，网络的结构是否是固定的

网络的层数是固定的，整个网络结构分为：初始层、搜索层以及最后层，其中初始层与最后层都是固定的，只有搜索层才是基于 DNAS 搜索算法算出来的，论文中设定的搜索层是 22 层，每一个层有 9 个 Block,https://github.com/AnnaAraslanova/FBNet/blob/master/supernet_functions/model_supernet.py#L33

- 损失函数 loss 在引入 latency 后是如何进行计算的


- 一次前向过程是搜索整个网络还是只随机采样一个 layer 的某个 block
- 网络搜索过程中有 2 个参数 theta 与 w ，这 2 个参数是如何进行更新的
参数的更新主要在 train_loop 这个函数中，2个参数也是采用轮流更新的方式，在更新 w 的时候，需要先固定 theta 参数，在更新 theta 的时候则需要先固定 w 参数P
- 搜索时间怎么样，能否直接在 ImageNet 上进行搜索
论文中说直接在 ImageNet 上进行搜索的，但是为了减少搜索的时间，只从原来的数据集的 1000 类别中选择了 100 类，而且只跑了 90 个 epoch,训练的过程一开始是训练 W 参数，后面是训练架构的概率参数 theta,W 用了 80% 的训练集，使用的是 SGD　函数优化，搜索完成后,从训练架构的概率分布 Pθ 中选择网络结构,从头开始训练他们。我们的架构搜索框架是在 pytorch 中实现的，搜索出来的模型是在 Caffe2 中训练的.
- 如何选择最终搜索的网络结构
- latency 是怎么计算出来的
论文中说了 latency 也是使用表格的方式，预先采集好这些 latency ,然后在搜索的过程中代入每个 op 的 latency
